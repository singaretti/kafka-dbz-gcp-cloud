===========================
= K A F K A | B R O K E R =
===========================

rpm --import https://packages.confluent.io/rpm/5.2/archive.key &&
echo '[Confluent.dist]' > /etc/yum.repos.d/confluent.repo &&
echo 'name=Confluent repository (dist)' >> /etc/yum.repos.d/confluent.repo &&
echo 'baseurl=https://packages.confluent.io/rpm/5.2/7' >> /etc/yum.repos.d/confluent.repo &&
echo 'gpgcheck=1' >> /etc/yum.repos.d/confluent.repo &&
echo 'gpgkey=https://packages.confluent.io/rpm/5.2/archive.key' >> /etc/yum.repos.d/confluent.repo &&
echo 'enabled=1' >> /etc/yum.repos.d/confluent.repo &&
echo '[Confluent]' >> /etc/yum.repos.d/confluent.repo &&
echo 'name=Confluent repository' >> /etc/yum.repos.d/confluent.repo &&
echo 'baseurl=https://packages.confluent.io/rpm/5.2' >> /etc/yum.repos.d/confluent.repo &&
echo 'gpgcheck=1' >> /etc/yum.repos.d/confluent.repo &&
echo 'gpgkey=https://packages.confluent.io/rpm/5.2/archive.key' >> /etc/yum.repos.d/confluent.repo &&
echo 'enabled=1' >> /etc/yum.repos.d/confluent.repo &&
echo 'vm.swappiness=0' | sudo tee --append /etc/sysctl.conf &&
yum clean all &&
yum update -y &&
yum install -y confluent-community-2.12 nc java-1.8.0-openjdk jq &&
echo 'LANG=en_US.utf-8' >> /etc/environment &&
echo 'LC_ALL=en_US.utf-8' >> /etc/environment &&
chown cp-kafka:confluent /var/log/confluent &&
chmod u+wx,g+wx,o= /var/log/confluent &&
mkdir -p -m 777 /data/kafka &&
cp /etc/kafka/server.properties /etc/kafka/server.properties.bkp &&
echo 'zookeeper.connect=zookeeper:2181/kafka' > /etc/kafka/server.properties &&
echo 'broker.id.generation.enable=true' >> /etc/kafka/server.properties &&
echo 'group.initial.rebalance.delay.ms=10000' >> /etc/kafka/server.properties &&
echo 'log.dirs=/data/kafka' >> /etc/kafka/server.properties &&
echo 'log.retention.hours=672' >> /etc/kafka/server.properties &&
echo 'log.segment.bytes=1073741824' >> /etc/kafka/server.properties &&
echo 'log.retention.check.interval.ms=300000' >> /etc/kafka/server.properties &&
echo 'confluent.support.metrics.enable=false' >> /etc/kafka/server.properties &&
echo 'delete.topic.enable=true' >> /etc/kafka/server.properties &&
echo 'num.partitions=1' >> /etc/kafka/server.properties &&
echo 'default.replication.factor=1' >> /etc/kafka/server.properties &&
echo 'min.insync.replicas=1' >> /etc/kafka/server.properties &&
echo 'offsets.topic.replication.factor=1' >> /etc/kafka/server.properties &&
echo 'zookeeper.connection.timeout.ms=10000' >> /etc/kafka/server.properties &&
echo 'auto.create.topics.enable=true' >> /etc/kafka/server.properties &&
echo 'transaction.state.log.replication.factor=1' >> /etc/kafka/server.properties &&
echo 'transaction.state.log.min.isr=1' >> /etc/kafka/server.properties &&
echo 'group.initial.rebalance.delay.ms=11000' >> /etc/kafka/server.properties &&
sed -i 's/-Xms256M -Xmx2G/-Xms256M -Xmx3G/g' /usr/bin/connect-distributed

================================
= S C H E M A  R E G I S T R Y =
================================

cp /etc/schema-registry/schema-registry.properties /etc/schema-registry/schema-registry.properties.bkp &&
echo 'listeners=http://0.0.0.0:8081 ' > /etc/schema-registry/schema-registry.properties &&
echo 'kafkastore.topic=schemas' >> /etc/schema-registry/schema-registry.properties &&
echo 'debug=false' >> /etc/schema-registry/schema-registry.properties &&
echo 'kafkastore.bootstrap.servers=PLAINTEXT://localhost:9092' >> /etc/schema-registry/schema-registry.properties

======================
= K A F K A  R E S T =
======================

cp /etc/kafka-rest/kafka-rest.properties /etc/kafka-rest/kafka-rest.properties.bkp &&
echo 'schema.registry.url=http://localhost:8081' > /etc/kafka-rest/kafka-rest.properties &&
echo 'bootstrap.servers=PLAINTEXT://localhost:9092' >> /etc/kafka-rest/kafka-rest.properties &&
echo 'id=kafka-rest-test-server1' >> /etc/kafka-rest/kafka-rest.properties &&
echo 'access.control.allow.methods=GET,POST,PUT,DELETE,OPTIONS' >> /etc/kafka-rest/kafka-rest.properties &&
echo 'access.control.allow.origin=*' >> /etc/kafka-rest/kafka-rest.properties

=============
= K A F K A =
=============

systemctl start confluent-kafka
systemctl stop confluent-kafka

systemctl status -l confluent-kafka
tail -f /var/log/kafka/server.log

================================
= S C H E M A  R E G I S T R Y =
================================

systemctl start confluent-schema-registry
systemctl stop confluent-schema-registry

kafka-configs --zookeeper localhost:2181/kafka --entity-type topics --entity-name schemas --alter --add-config cleanup.policy="compact"

systemctl status -l confluent-schema-registry
tail -f /var/log/confluent/schema-registry/schema-registry.log

==================
= R E S T  A P I =
==================

systemctl start confluent-kafka-rest
systemctl stop confluent-kafka-rest

systemctl status -l confluent-kafka-rest
tail -f /var/log/confluent/kafka-rest/kafka-rest.log

==================================
= B R O K E R  | T E R M I N A L =
==================================

# create a topic
kafka-topics --zookeeper zookeeper:2181/kafka --create --topic teste --replication-factor 1 --partitions 1

# produce data to the topic
kafka-console-producer --broker-list kafka:9092 --topic teste

# check consumer group
kafka-consumer-groups --bootstrap-server kafka:9092 --list

# describe topic/group
kafka-consumer-groups --bootstrap-server kafka:9092 --describe --group grupo

# create topic with consumer group
kafka-topics --zookeeper zookeeper:2181/kafka --create --topic teste --group grupo-teste --replication-factor 1 --partitions 1

# read avro format messages from PostgreSQL
kafka-avro-console-consumer --bootstrap-server kafka:9092 --topic postgres_server.postgres_schema.postgres_table

# read that data from beginning
kafka-avro-console-consumer --bootstrap-server kafka:9092 --topic postgres_server.postgres_schema.postgres_table

# list kafka topics
kafka-topics --list --zookeeper zookeeper:2181/kafka

# delete kafka topic
kafka-topics --zookeeper zookeeper:2181/kafka --delete --topic test

# describe the topic
kafka-log-dirs --describe --bootstrap-server kafka:9092 --topic-list test

# test kafka topic - generate 10KB of random data (stress test)
base64 /dev/urandom | head -c 10000 | egrep -ao "\w" | tr -d '\n' > file10KB.txt

kafka-producer-perf-test --topic test --num-records 10000000 --throughput 20 --payload-file file10KB.txt --producer-props acks=1 bootstrap.servers=kafka:9092 --payload-delimiter A

# offset of the topic (~ count(*))
kafka-run-class kafka.tools.GetOffsetShell --broker-list kafka:9092 --topic YYYYYY --time -1 --offsets 1 | awk -F  ":" '{sum += $3} END {print sum}'

# offset per partitions
kafka-run-class kafka.tools.GetOffsetShell --broker-list kafka:9092 --topic YYYYYY

============================
= V A L I D A T E  A V R O =
============================

aws s3 cp s3://avro_file.avro .
http://mirror.metrocast.net/apache/avro/avro-1.8.2/java/avro-tools-1.8.2.jar
java -jar avro-tools-1.8.2.jar tojson avro_file.avro